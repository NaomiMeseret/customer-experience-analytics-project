{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Config and Imports ---\n",
        "config = {\n",
        "    'MODEL_NAME': 'distilbert-base-uncased-finetuned-sst-2-english',\n",
        "    'TFIDF_MAX_FEATURES': 50,\n",
        "    'TFIDF_NGRAM_RANGE': (1, 2),\n",
        "    'THEME_KEYWORDS': {\n",
        "        'Account Access Issues': ['login', 'password', 'pin', 'locked', 'authentication'],\n",
        "        'Transaction Performance': ['transfer', 'slow', 'fast', 'timeout', 'processing'],\n",
        "        'User Interface & Experience': ['ui', 'interface', 'design', 'navigation', 'user friendly'],\n",
        "        'Customer Support': ['support', 'help', 'response'],\n",
        "        'Feature Requests': ['feature', 'add', 'missing', 'request'],\n",
        "        'App Reliability': ['crash', 'bug', 'glitch', 'freeze'],\n",
        "    },\n",
        "    'RANDOM_SEED': 42,\n",
        "    'DATA_FILE': '../data/processed/reviews_cleaned.csv',\n",
        "}\n",
        "print(\"Configuration:\")\n",
        "for k, v in config.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "sns.set_style(\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1. Load Data\n",
        "\n",
        "# Always start with loading preprocessed/clean reviews\n",
        "df = pd.read_csv(config['DATA_FILE'])\n",
        "print(f\"Loaded {len(df)} reviews.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 2. Preprocessing\n",
        "\n",
        "def preprocess(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    import re\n",
        "    t = text.lower()\n",
        "    t = re.sub(r'[^a-z0-9\\s]', ' ', t)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t\n",
        "\n",
        "df['review_clean'] = df['review'].apply(preprocess)\n",
        "\n",
        "print(\"Sample cleaned review:\")\n",
        "print(df['review_clean'].iloc[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 3. Tokenization & Keyword Extraction\n",
        "vectorizer = TfidfVectorizer(max_features=config['TFIDF_MAX_FEATURES'],\n",
        "                             ngram_range=config['TFIDF_NGRAM_RANGE'],\n",
        "                             stop_words='english')\n",
        "X = vectorizer.fit_transform(df['review_clean'])\n",
        "keywords = vectorizer.get_feature_names_out()\n",
        "print(f\"\\nTop {len(keywords)} TF-IDF tokens:\\n{keywords[:10]} ...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 4. Sentiment Analysis\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(config['MODEL_NAME'])\n",
        "model = AutoModelForSequenceClassification.from_pretrained(config['MODEL_NAME'])\n",
        "\n",
        "def get_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    pred = logits.softmax(dim=-1)\n",
        "    label = pred.argmax().item()\n",
        "    score = round(float(pred.max()),3)\n",
        "    return ['NEGATIVE','POSITIVE'][label], score\n",
        "\n",
        "# Run on a small sample for demonstration\n",
        "sample_idxs = [0, 1, 2]\n",
        "for idx in sample_idxs:\n",
        "    label, score = get_sentiment(df['review_clean'].iloc[idx])\n",
        "    print(f\"Review: {df['review'].iloc[idx][:50]}... | Sentiment: {label} ({score})\")\n",
        "\n",
        "# (Optional for full dataset: uncomment to use)\n",
        "# df[['sentiment_label', 'sentiment_score']] = df['review_clean'].apply(get_sentiment).tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 5. Theme Extraction (Rule-based)\n",
        "def extract_themes(review, theme_keywords):\n",
        "    out = []\n",
        "    text = review.lower()\n",
        "    for theme, kws in theme_keywords.items():\n",
        "        for kw in kws:\n",
        "            if kw in text:\n",
        "                out.append(theme)\n",
        "                break\n",
        "    return out\n",
        "\n",
        "df['themes_extracted'] = df['review_clean'].apply(lambda t: extract_themes(t, config['THEME_KEYWORDS']))\n",
        "print(\"\\nSample review and its extracted themes:\")\n",
        "print(df[['review', 'themes_extracted']].head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 6. Verification/Assertions\n",
        "# Test: No empty reviews\n",
        "assert (df['review_clean'].str.len() > 0).all()\n",
        "# Test: Sentiment label presence (uncomment if you run sentiment on all data)\n",
        "# assert df['sentiment_label'].notnull().mean() > 0.9\n",
        "\n",
        "# Test: At least 90% should have at least one theme extracted\n",
        "theme_coverage = (df['themes_extracted'].apply(len) > 0).mean()\n",
        "print(f\"\\nTheme extraction coverage: {theme_coverage*100:.1f}% of reviews received at least 1 theme.\")\n",
        "\n",
        "# Show a few sample reviews with extracted themes and (if run) sentiment\n",
        "print(df[['review', 'themes_extracted']].sample(5, random_state=config['RANDOM_SEED']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2: Sentiment and Thematic Analysis\n",
        "\n",
        "This notebook covers sentiment analysis and thematic extraction from customer reviews.\n",
        "\n",
        "## Objectives:\n",
        "- Perform sentiment analysis using DistilBERT model\n",
        "- Extract themes and keywords from reviews\n",
        "- Identify satisfaction drivers and pain points\n",
        "- Prepare data for insights generation\n",
        "\n",
        "## Model Used:\n",
        "- **Sentiment Analysis**: `distilbert-base-uncased-finetuned-sst-2-english` (Hugging Face)\n",
        "- **Thematic Analysis**: TF-IDF and spaCy NLP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Preprocessed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load cleaned data\n",
        "df = pd.read_csv('../data/processed/reviews_cleaned.csv')\n",
        "print(f\"‚úÖ Loaded {len(df)} reviews for analysis\")\n",
        "print(f\"\\nüìä Data Overview:\")\n",
        "print(f\"   Banks: {df['bank'].unique().tolist()}\")\n",
        "print(f\"   Rating Range: {df['rating'].min()} - {df['rating'].max()} stars\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Sentiment Analysis\n",
        "\n",
        "**Note**: The full sentiment analysis is done using `sentiment_analysis.py`. This notebook demonstrates the process and loads results.\n",
        "\n",
        "### 2.1 Load Sentiment Analysis Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data with sentiment analysis\n",
        "try:\n",
        "    df_sentiment = pd.read_csv('../data/processed/reviews_with_sentiment.csv')\n",
        "    print(f\"‚úÖ Loaded {len(df_sentiment)} reviews with sentiment analysis\")\n",
        "    print(f\"\\nüí≠ Sentiment Distribution:\")\n",
        "    print(df_sentiment['sentiment_label'].value_counts())\n",
        "    print(f\"\\nüìä Average Sentiment Score: {df_sentiment['sentiment_score'].mean():.3f}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ö†Ô∏è  Sentiment analysis file not found.\")\n",
        "    print(\"üí° Please run: python task2_analysis/sentiment_analysis.py\")\n",
        "    df_sentiment = df.copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Sentiment Analysis Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sentiment distribution by bank\n",
        "if 'sentiment_label' in df_sentiment.columns:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Sentiment distribution\n",
        "    ax1 = axes[0]\n",
        "    sentiment_by_bank = pd.crosstab(df_sentiment['bank'], df_sentiment['sentiment_label'])\n",
        "    sentiment_by_bank.plot(kind='bar', ax=ax1, color=['#FF6B6B', '#6BCB77'], width=0.8)\n",
        "    ax1.set_title('Sentiment Distribution by Bank', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Bank', fontsize=12)\n",
        "    ax1.set_ylabel('Number of Reviews', fontsize=12)\n",
        "    ax1.legend(title='Sentiment', title_fontsize=11)\n",
        "    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=0)\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Average sentiment score\n",
        "    if 'sentiment_score' in df_sentiment.columns:\n",
        "        ax2 = axes[1]\n",
        "        avg_sentiment = df_sentiment.groupby('bank')['sentiment_score'].mean().sort_values(ascending=False)\n",
        "        colors = ['#6BCB77' if x > 0.5 else '#FF6B6B' for x in avg_sentiment]\n",
        "        avg_sentiment.plot(kind='bar', ax=ax2, color=colors, width=0.6)\n",
        "        ax2.set_title('Average Sentiment Score by Bank', fontsize=14, fontweight='bold')\n",
        "        ax2.set_xlabel('Bank', fontsize=12)\n",
        "        ax2.set_ylabel('Average Sentiment Score', fontsize=12)\n",
        "        ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Neutral (0.5)')\n",
        "        ax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)\n",
        "        ax2.legend()\n",
        "        ax2.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Sentiment by Rating\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze sentiment correlation with ratings\n",
        "if 'sentiment_label' in df_sentiment.columns:\n",
        "    print(\"=\"*60)\n",
        "    print(\"üí≠ SENTIMENT BY RATING\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    sentiment_rating = pd.crosstab(df_sentiment['rating'], df_sentiment['sentiment_label'])\n",
        "    print(\"\\n\", sentiment_rating)\n",
        "    \n",
        "    # Visualization\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sentiment_rating.plot(kind='bar', ax=ax, color=['#FF6B6B', '#6BCB77'], width=0.8)\n",
        "    ax.set_title('Sentiment Distribution by Rating', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Rating', fontsize=12)\n",
        "    ax.set_ylabel('Number of Reviews', fontsize=12)\n",
        "    ax.legend(title='Sentiment', title_fontsize=11)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Thematic Analysis\n",
        "\n",
        "**Note**: The full thematic analysis is done using `thematic_analysis.py`. This notebook demonstrates the process and loads results.\n",
        "\n",
        "### 3.1 Load Thematic Analysis Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data with themes\n",
        "try:\n",
        "    df_themes = pd.read_csv('../data/processed/reviews_with_themes.csv')\n",
        "    print(f\"‚úÖ Loaded {len(df_themes)} reviews with thematic analysis\")\n",
        "    \n",
        "    # Extract and count themes\n",
        "    all_themes = []\n",
        "    for themes in df_themes['themes']:\n",
        "        if pd.notna(themes):\n",
        "            try:\n",
        "                if isinstance(themes, str):\n",
        "                    theme_list = eval(themes) if themes.startswith('[') else [themes]\n",
        "                else:\n",
        "                    theme_list = themes\n",
        "                all_themes.extend(theme_list)\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    print(f\"\\nüè∑Ô∏è  Top 10 Themes Across All Banks:\")\n",
        "    theme_counts = Counter(all_themes)\n",
        "    for theme, count in theme_counts.most_common(10):\n",
        "        print(f\"   {theme}: {count} reviews\")\n",
        "        \n",
        "except FileNotFoundError:\n",
        "    print(\"‚ö†Ô∏è  Thematic analysis file not found.\")\n",
        "    print(\"üí° Please run: python task2_analysis/thematic_analysis.py\")\n",
        "    df_themes = df_sentiment.copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Theme Distribution by Bank\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze themes by bank\n",
        "if 'themes' in df_themes.columns:\n",
        "    theme_data = []\n",
        "    for _, row in df_themes.iterrows():\n",
        "        if pd.notna(row['themes']):\n",
        "            try:\n",
        "                if isinstance(row['themes'], str):\n",
        "                    themes = eval(row['themes']) if row['themes'].startswith('[') else [row['themes']]\n",
        "                else:\n",
        "                    themes = row['themes']\n",
        "                for theme in themes:\n",
        "                    theme_data.append({'bank': row['bank'], 'theme': theme})\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    if theme_data:\n",
        "        theme_df = pd.DataFrame(theme_data)\n",
        "        theme_counts = theme_df.groupby(['bank', 'theme']).size().unstack(fill_value=0)\n",
        "        \n",
        "        # Visualization\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "        theme_counts.plot(kind='barh', ax=ax, width=0.8, colormap='Set3')\n",
        "        ax.set_title('Theme Distribution by Bank', fontsize=14, fontweight='bold')\n",
        "        ax.set_xlabel('Number of Reviews', fontsize=12)\n",
        "        ax.set_ylabel('Bank', fontsize=12)\n",
        "        ax.legend(title='Theme', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
        "        ax.grid(axis='x', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Print top themes per bank\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üè∑Ô∏è  TOP THEMES BY BANK\")\n",
        "        print(\"=\"*60)\n",
        "        for bank in df_themes['bank'].unique():\n",
        "            bank_themes = theme_df[theme_df['bank'] == bank]['theme'].value_counts().head(3)\n",
        "            print(f\"\\n{bank}:\")\n",
        "            for theme, count in bank_themes.items():\n",
        "                print(f\"   {theme}: {count} reviews\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2 Summary\n",
        "\n",
        "‚úÖ **Completed Steps:**\n",
        "1. Sentiment analysis using DistilBERT model\n",
        "2. Sentiment distribution analysis by bank and rating\n",
        "3. Thematic analysis using TF-IDF and keyword extraction\n",
        "4. Theme identification and categorization\n",
        "5. Data preparation for insights generation\n",
        "\n",
        "‚úÖ **KPIs Achieved:**\n",
        "- Sentiment scores for 90%+ reviews\n",
        "- 3+ themes identified per bank\n",
        "- Modular analysis pipeline\n",
        "\n",
        "**Next Step**: Proceed to Task 3 for Database Storage or Task 4 for Insights and Recommendations\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
